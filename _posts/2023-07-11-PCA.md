---
title: "[개념정리] PCA"
categories:
 - DeepLearning
tags:
 - DeepLearning
 - PCA
 - 주성분분석
use_math: true
last_modified_at: 2023-12-27
sidebar:
  nav: "sidebar-categories"
permalink: /deeplearning/pca/
toc: true
toc_sticky: true
toc_label: "PCA"
---
## Purpose

데이터의 분산을 최대로 하는 principal axis를 구하고 고차원 데이터를 저차원으로 차원 축소, projection

## How

$X$라는 데이터(행렬)이 있다고 가정하자. $X$의 column vector는 각 feature를 의미하고 row vector는 각 sample data(예를 들어 각 사람 한명 한명)이다. 여기서 각 column vector를 각각의 평균을 빼주면 $X$는 데이터의 분산, variance를 element로 하는 행렬이 된다. 이때, $X^TX$를 하면 결과, Output 행렬은 각 벡터의 내적을 element로 하는 행렬이 되는데, 이는 편차의 제곱, 즉 각 데이터의 변화가 얼마나 닮았는지를 나타낸다. 그 결과로 우리는 데이터의 형태, 분포를 알 수 있고 이를 행렬로 표현할 수 있다. 정리하면 $(X^TX)_{ij}$는 i번째 feature와 j번째 feature의 변동이 닮은 정도를 나타낸다.(서로 함께 변하는 정도)

PCA를 수행하려면 먼저 input 데이터의 covariance matrix를 구한 다음, eigen decomposition을 통해 covariance matrix의 eigen value와 eigen vector를 계산해야 한다. 그런 다음 이를 정렬하고 축소하고 싶은 차원의 수 만큼 선택하여 projection하면 차원 축소를 수행할 수 있다.

**Eigen value는 principal component eigen vector가 데이터에서 capture하는 variance의 양을 나타낸다**. 

## Dimension Reduction

N차원 데이터는 N개의 eigen vector가 나오는데, 그 중에서 eigen value가 큰 M개의 eigen vector를 골라 이들이 이루는 공간으로 데이터를 projection하여 나타내면 N차원에서 M차원으로 Dimension Reduction.
