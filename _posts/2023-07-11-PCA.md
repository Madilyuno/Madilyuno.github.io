---
title: "[개념정리] PCA"
categories:
 - DeepLearning
tags:
 - DeepLearning
 - PCA
 - 주성분분석
last_modified_at: 2023-07-11
sidebar:
  nav: "sidebar-categories"
permalink: /deeplearning/pca/
toc: true
toc_sticky: true
toc_label: "PCA"
---
## Purpose

데이터의 분산을 최대로 하는 principal axis를 구하고 고차원 데이터를 저차원으로 차원 축소, projection

## How

$X$라는 데이터(행렬)이 있다고 가정하자. $X$의 column vector는 각 feature를 의미하고 row vector는 각 sample data(예를 들어 각 사람 한명 한명)이다. 여기서 각 column vector를 각각의 평균을 빼주면 $X$는 데이터의 분산, variance를 element로 하는 행렬이 된다. 이때, $X^TX$를 하면 결과, Output 행렬은 각 벡터의 내적을 element로 하는 행렬이 되는데, 이는 편차의 제곱, 즉 각 데이터의 변화가 얼마나 닮았는지를 나타낸다. 그 결과로 우리는 데이터의 형태, 분포를 알 수 있고 이를 행렬로 표현할 수 있다. 정리하면 $(X^TX)_{ij}$는 i번째 feature와 j번째 feature의 변동이 닮은 정도를 나타낸다.(서로 함께 변하는 정도)

## Dimension Reduction

N차원 데이터는 N개의 eigen vector가 나오는데, 그 중에서 eigen value가 큰 M개의 eigen vector를 골라 이들이 이루는 공간으로 데이터를 projection하여 나타내면 N차원에서 M차원으로 Dimension Reduction.
